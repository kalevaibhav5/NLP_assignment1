{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bfedef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your first String to compare: Spokesman confirms\n",
      "Enter your second String to compare: Spokeswoman said\n",
      "Enter your insert cost: 1\n",
      "Enter your delete cost: 1\n",
      "Enter your Substitute cost: 2\n",
      "\n",
      "distance matrix is :\n",
      " 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 \n",
      " 1  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 \n",
      " 2  1  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 \n",
      " 3  2  1  0  1  2  3  4  5  6  7  8  9 10 11 12 13 \n",
      " 4  3  2  1  0  1  2  3  4  5  6  7  8  9 10 11 12 \n",
      " 5  4  3  2  1  0  1  2  3  4  5  6  7  8  9 10 11 \n",
      " 6  5  4  3  2  1  0  1  2  3  4  5  6  7  8  9 10 \n",
      " 7  6  5  4  3  2  1  2  3  2  3  4  5  6  7  8  9 \n",
      " 8  7  6  5  4  3  2  3  4  3  2  3  4  5  6  7  8 \n",
      " 9  8  7  6  5  4  3  4  5  4  3  2  3  4  5  6  7 \n",
      "10  9  8  7  6  5  4  5  6  5  4  3  2  3  4  5  6 \n",
      "11 10  9  8  7  6  5  6  7  6  5  4  3  4  5  6  7 \n",
      "12 11 10  9  8  7  6  7  6  7  6  5  4  5  6  7  8 \n",
      "13 12 11 10  9  8  7  8  7  8  7  6  5  6  7  8  9 \n",
      "14 13 12 11 10  9  8  9  8  9  8  7  6  7  8  9 10 \n",
      "15 14 13 12 11 10  9 10  9 10  9  8  7  8  9  8  9 \n",
      "16 15 14 13 12 11 10 11 10 11 10  9  8  9 10  9 10 \n",
      "17 16 15 14 13 12 11 12 11 10 11 10  9 10 11 10 11 \n",
      "18 17 16 15 14 13 12 13 12 11 12 11 10  9 10 11 12 \n",
      "\n",
      "minimum edit distance between the strings \"Spokesman confirms\" and \"Spokeswoman said\" is 12\n"
     ]
    }
   ],
   "source": [
    "# Solution of question 7 in NLP assignment 1\n",
    "# This program accepts the two strings to compare and find the minimum edit distance to convert the string to each other.\n",
    "# It also accepts flexible cost for the insert, delete and substitute operation\n",
    "\n",
    "# accept the inputs\n",
    "String1 = input(\"Enter your first String to compare: \")\n",
    "String2 = input(\"Enter your second String to compare: \")\n",
    "insertCost = int(input(\"Enter your insert cost: \"))\n",
    "deleteCost = int(input(\"Enter your delete cost: \"))\n",
    "substituteCost = int(input(\"Enter your Substitute cost: \"))\n",
    "\n",
    "# get the input strings in the list of characters to perform algorithm\n",
    "Str1 = list(String1)\n",
    "Str2 = list(String2)\n",
    "\n",
    "# print new line\n",
    "print()\n",
    "\n",
    "# this function takes in all the inputs on the consoles and prints the distance matrix and also gives the minimum distance\n",
    "def editMinimumEditDistance(Str1, Str2, insertCost, deleteCost, substituteCost):\n",
    "    # Initialize Distance Matrix. determine the length of given strings\n",
    "    # length of String1 will be number of rows in the matrix\n",
    "    # length of String2 will be number of columns in the matrix\n",
    "    cols, rows = (len(Str2) + 1, len(Str1) + 1)\n",
    "\n",
    "    D = [[0] * cols for i in range(rows)]\n",
    "\n",
    "    # base condition. D(i,0) = i and D(0,j) = j\n",
    "    for i in range(rows):\n",
    "        D[i][0] = i\n",
    "\n",
    "    for j in range(cols):\n",
    "        D[0][j] = j\n",
    "\n",
    "    # applying the algorithm to calculate the distance matrix by finding the minimum of insert, delete or substitute operation for each pass\n",
    "    for i in range(1, rows):\n",
    "        for j in range(1, cols):\n",
    "            if Str1[i - 1] == Str2[j - 1]:\n",
    "                sub_cost = 0\n",
    "            else:\n",
    "                sub_cost = substituteCost\n",
    "            D[i][j] = min((D[i - 1][j] + deleteCost), (D[i][j - 1] + insertCost), (D[i - 1][j - 1] + sub_cost))\n",
    "\n",
    "    # print the distance matrix\n",
    "    print(\"distance matrix is :\")\n",
    "    for row in D:\n",
    "        for col in row:\n",
    "            print(f'{col:>2}', end=\" \")\n",
    "        print()\n",
    "\n",
    "    print()\n",
    "\n",
    "    # return the minimum distance as the bottom right column of the matrix\n",
    "    return D[rows - 1][cols - 1]\n",
    "\n",
    "# print the minimum distance the inputted strings\n",
    "print(\"minimum edit distance between the strings \\\"\" + String1 + \"\\\" and \\\"\" + String2 + \"\\\" is \" + str(\n",
    "    editMinimumEditDistance(Str1, Str2, insertCost, deleteCost, substituteCost)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf07b8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in corpus are 47\n",
      "Unique tokens in corpus are 36\n",
      "\n",
      "The unigram Frequencies are as below\n",
      "{'the': 6, 'day': 1, 'was': 1, 'grey': 1, 'and': 3, 'bitter': 1, 'cold': 1, ',': 3, 'dogs': 1, 'would': 1, 'not': 1, 'take': 1, 'scent': 1, '.': 2, 'big': 1, 'black': 1, 'bitch': 1, 'had': 1, 'taken': 1, 'one': 1, 'sniff': 1, 'at': 1, 'bear': 1, 'tracks': 1, 'backed': 1, 'off': 1, 'skulked': 1, 'back': 1, 'to': 1, 'pack': 1, 'with': 1, 'her': 2, 'tail': 1, 'between': 1, 'legs': 1, '$': 1}\n",
      "\n",
      "\n",
      "Bigram Language Model Probabilities are as below: \n",
      "\n",
      "P(day | the) = 1/6\n",
      "P(was | day) = 1/1\n",
      "P(grey | was) = 1/1\n",
      "P(and | grey) = 1/1\n",
      "P(bitter | and) = 1/3\n",
      "P(cold | bitter) = 1/1\n",
      "P(, | cold) = 1/1\n",
      "P(and | ,) = 2/3\n",
      "P(the | and) = 1/3\n",
      "P(dogs | the) = 1/6\n",
      "P(would | dogs) = 1/1\n",
      "P(not | would) = 1/1\n",
      "P(take | not) = 1/1\n",
      "P(the | take) = 1/1\n",
      "P(scent | the) = 1/6\n",
      "P(. | scent) = 1/1\n",
      "P(the | .) = 1/2\n",
      "P(big | the) = 1/6\n",
      "P(black | big) = 1/1\n",
      "P(bitch | black) = 1/1\n",
      "P(had | bitch) = 1/1\n",
      "P(taken | had) = 1/1\n",
      "P(one | taken) = 1/1\n",
      "P(sniff | one) = 1/1\n",
      "P(at | sniff) = 1/1\n",
      "P(the | at) = 1/1\n",
      "P(bear | the) = 1/6\n",
      "P(tracks | bear) = 1/1\n",
      "P(, | tracks) = 1/1\n",
      "P(backed | ,) = 1/3\n",
      "P(off | backed) = 1/1\n",
      "P(, | off) = 1/1\n",
      "P(skulked | and) = 1/3\n",
      "P(back | skulked) = 1/1\n",
      "P(to | back) = 1/1\n",
      "P(the | to) = 1/1\n",
      "P(pack | the) = 1/6\n",
      "P(with | pack) = 1/1\n",
      "P(her | with) = 1/1\n",
      "P(tail | her) = 1/2\n",
      "P(between | tail) = 1/1\n",
      "P(her | between) = 1/1\n",
      "P(legs | her) = 1/2\n",
      "P(. | legs) = 1/1\n",
      "P($ | .) = 1/2\n",
      "\n",
      "Enter some initial starting word to generate the full text: the day was\n",
      "\n",
      "The generated text is: \n",
      "the day was grey and bitter cold , backed off , and the day was dogs would not take the scent . the big black bitch had taken one sniff at the bear tracks , pack with her tail between her legs . $ \n"
     ]
    }
   ],
   "source": [
    "# Solution of question 8 in NLP assignment 1\n",
    "# This program takes as input intial word from user and generate the text using the bigram language model which is also calculated in the program.\n",
    "# this program can take any text string as corpus. Change the text in the variable corpus_text to do so.\n",
    "# import nltk library for text processing, tokenization and pandas library for data manipulation\n",
    "import nltk\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "# this function get the last word as initial search word from bigram probabilities\n",
    "def findWord(str):\n",
    "    words = str.split()\n",
    "    return words[-1]\n",
    "\n",
    "# corpus text to build the unigram model. Added $ as special end character to help terminate the text generation\n",
    "corpus_text = \"The day was grey and bitter cold, and the dogs would not take the scent. The big black bitch had taken one sniff at the bear tracks, backed off, and skulked back to the pack with her tail between her legs. $\"\n",
    "\n",
    "def tokenGenration(text):\n",
    "    # generate the tokens\n",
    "    tokens = nltk.word_tokenize(corpus_text.lower())\n",
    "\n",
    "    # print the total tokens\n",
    "    print(\"Total tokens in corpus are \" + str(len(tokens)))\n",
    "\n",
    "    # print the total unique tokens (types) numbers\n",
    "    print(\"Unique tokens in corpus are \" + str((len(set(tokens)))) + \"\\n\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# generate the tokens from corpus using the method.\n",
    "tokens = tokenGenration(corpus_text)\n",
    "\n",
    "# function to calculate unigram frequencies\n",
    "def unigramFrequency(tokens):\n",
    "    # using Counter to find frequency of elements\n",
    "    frequency = collections.Counter(tokens)\n",
    "\n",
    "    # printing the frequency of unique tokens (unigrams)\n",
    "    print(\"The unigram Frequencies are as below\")\n",
    "    print(dict(frequency))\n",
    "    print(\"\\n\")\n",
    "    return frequency\n",
    "\n",
    "# make a call to unigram frequencies\n",
    "frequency = unigramFrequency(tokens)\n",
    "\n",
    "# function to calculate , print bigram language model\n",
    "def bigramLanguageModel(tokens,frequency):\n",
    "    #Creating the bigrams using nltk bigrams function\n",
    "    bgs = nltk.bigrams(tokens)\n",
    "\n",
    "    #computing frequency distribution for all the bigrams in the text\n",
    "    fdist = nltk.FreqDist(bgs)\n",
    "\n",
    "    # printing the probability of the bigrams\n",
    "    print(\"Bigram Language Model Probabilities are as below: \\n\")\n",
    "    for k,v in fdist.items():\n",
    "        print(\"P(\" + str(k[1]) + \" | \" + str(k[0]) + \") = \" + str(v) + \"/\" + str(frequency[k[0]]))\n",
    "\n",
    "    # create the dataframe with the bigram language model columns\n",
    "    df = pd.DataFrame(columns = [\"word\",\"prev_word\", \"Probability\"])\n",
    "\n",
    "    # append the rows in dataframe with the probabilities\n",
    "    for k,v in fdist.items():\n",
    "        p = v/frequency[k[0]]\n",
    "        df.loc[len(df.index)] = [k[1],k[0],p]\n",
    "\n",
    "    # sort the bigram language model dataframe in order of probabilties\n",
    "    sorted_df = df.sort_values(by=['Probability'], ascending=False, ignore_index=True, inplace=False)\n",
    "\n",
    "    return sorted_df\n",
    "\n",
    "# get the dataframe having bigram probablity frequencies. Columns word, previous word and probability\n",
    "sorted_df = bigramLanguageModel(tokens,frequency)\n",
    "\n",
    "\n",
    "# function to generate the text using the bigram probabilities\n",
    "def generateText(sorted_df):\n",
    "\n",
    "    # iterator local variables\n",
    "    itr=0\n",
    "    indx = -1\n",
    "\n",
    "    # get the initial few words as input\n",
    "    input_string = input(\"\\nEnter some initial starting word to generate the full text: \")\n",
    "\n",
    "    # call the findWord to get the last word of the input as initial seach word\n",
    "    find_text = findWord(input_string)\n",
    "\n",
    "    # generated text also comprises of initial words so append it to the final text\n",
    "    generated_text = input_string + \" \"\n",
    "\n",
    "    # counter and used_index to avoid any infinte loops\n",
    "    counter = 0\n",
    "    used_index = [-1]\n",
    "\n",
    "    # check the next word with highest probability ( already sorted) and append the highest probable word to generated text.\n",
    "    # this found word will be next seach word. This cycle continues until end of the text '$' is found or all the bigram entries are looped through.\n",
    "    while sorted_df.word[itr] != \"$\" and counter <= len(sorted_df.word):\n",
    "        for i in range(len(sorted_df.word)):\n",
    "            if find_text == sorted_df.prev_word[i]:\n",
    "                if i not in used_index:\n",
    "                    indx = i\n",
    "                if i in used_index:\n",
    "                    find_text = sorted_df.word[i]\n",
    "        if indx not in used_index:\n",
    "            generated_text = generated_text + sorted_df.word[indx] + \" \"\n",
    "            itr=indx\n",
    "            used_index.append(indx)\n",
    "            find_text = sorted_df.word[indx]\n",
    "        counter = counter + 1\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# print the final generated text to console\n",
    "\n",
    "print(\"\\nThe generated text is: \\n\" + generateText(sorted_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69faa9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
